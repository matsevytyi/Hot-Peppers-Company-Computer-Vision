{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309621d949982fc9",
   "metadata": {},
   "source": [
    "# Train COCO Base (Mamba-Vision)\n",
    "\n",
    "Notebook-first pipeline with safety gates:\n",
    "\n",
    "- dependency checks\n",
    "- optional FiftyOne export + manifest generation\n",
    "- one-batch shape/loss sanity\n",
    "- mandatory pilot before full run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a337582cbf7cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:28:14.543338Z",
     "start_time": "2026-02-09T12:28:13.753836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision\n",
      "Run mode: pilot\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in (start, *start.parents):\n",
    "        if (candidate / '.git').exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "RUN_MODE = 'pilot'  # pilot | full\n",
    "PREPARE_DATA = False\n",
    "AUTO_CONFIRM_FULL = False\n",
    "\n",
    "print('Repo root:', REPO_ROOT)\n",
    "print('Run mode:', RUN_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0fd705b535e42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:28:15.430264Z",
     "start_time": "2026-02-09T12:28:15.426743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/configs/training/coco_base.yaml')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_PATH = REPO_ROOT / 'configs/training/coco_base.yaml'\n",
    "CONFIG_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4eaf8d8976789b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:28:17.184557Z",
     "start_time": "2026-02-09T12:28:17.153493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch': True,\n",
       " 'torchvision': True,\n",
       " 'yaml': True,\n",
       " 'safetensors': True,\n",
       " 'tqdm': True,\n",
       " 'einops': True,\n",
       " 'fiftyone': True,\n",
       " 'mambavision': True,\n",
       " 'wandb': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipelines.dependencies import check_packages, assert_required_packages\n",
    "\n",
    "required = ['torch', 'torchvision', 'yaml', 'safetensors', 'tqdm', 'einops']\n",
    "optional = ['fiftyone', 'mambavision', 'wandb']\n",
    "status = check_packages(required + optional)\n",
    "status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a7ee0f9745dde9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:30:20.079214Z",
     "start_time": "2026-02-09T12:30:20.070439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core dependencies look good\n"
     ]
    }
   ],
   "source": [
    "assert_required_packages(['torch', 'torchvision', 'yaml', 'safetensors', 'einops'])\n",
    "print('Core dependencies look good')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7887416c5ca1a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:30:20.522511Z",
     "start_time": "2026-02-09T12:30:20.511882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(run_name='coco_base', data={'source': 'fiftyone_zoo', 'zoo_name': 'coco-2017', 'split_train': 'train', 'split_val': 'validation', 'manifest_train': 'configs/manifests/coco_train.json', 'manifest_val': 'configs/manifests/coco_val.json', 'export_train_dir': 'data/exports/coco/train', 'export_val_dir': 'data/exports/coco/val', 'max_samples_train': None, 'max_samples_val': None}, model=ModelSection(backbone='mamba_vision_T2', num_classes=8, pretrained=True, checkpoint_path='mambavision_tiny2_1k.pth.tar', base_checkpoint='', model_file='/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/mamba-vision-ours/model.py'), train=TrainSection(epochs=30, batch_size=8, num_workers=4, lr=0.0001, weight_decay=0.0001, scheduler='cosine', pilot_steps=5, pilot_val_steps=2, precision='fp16', device='cuda', image_size=640, grad_clip_norm=1.0), ckpt=CkptConfig(output_path='/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/base/coco_base.ckpt', save_top_k=3, save_last=True), lora=None, freeze=FreezeConfig(backbone_base=False, neck=False, head=False))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pipelines.contracts import TrainConfig\n",
    "\n",
    "cfg = TrainConfig.from_yaml(CONFIG_PATH)\n",
    "cfg.model.model_file = str((REPO_ROOT / cfg.model.model_file).resolve())\n",
    "cfg.ckpt.output_path = str((REPO_ROOT / cfg.ckpt.output_path).resolve())\n",
    "if cfg.model.base_checkpoint:\n",
    "    cfg.model.base_checkpoint = str((REPO_ROOT / cfg.model.base_checkpoint).resolve())\n",
    "if cfg.data.get('lora_output_path'):\n",
    "    cfg.data['lora_output_path'] = str((REPO_ROOT / cfg.data['lora_output_path']).resolve())\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c79b4358c435d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:30:21.307246Z",
     "start_time": "2026-02-09T12:30:21.299966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPARE_DATA=False -> expecting existing manifests\n"
     ]
    }
   ],
   "source": [
    "from pipelines.fiftyone_data import prepare_zoo_split_export\n",
    "\n",
    "if PREPARE_DATA:\n",
    "    print('Preparing train split export...')\n",
    "    prepare_zoo_split_export(\n",
    "        zoo_name=cfg.data['zoo_name'],\n",
    "        split=cfg.data['split_train'],\n",
    "        dataset_name=f\"{cfg.run_name}_{cfg.data['split_train']}\",\n",
    "        export_dir=str(REPO_ROOT / cfg.data['export_train_dir']),\n",
    "        manifest_path=str(REPO_ROOT / cfg.data['manifest_train']),\n",
    "        source=cfg.data.get('source', 'fiftyone_zoo'),\n",
    "        max_samples=cfg.data.get('max_samples_train'),\n",
    "        time_of_day=cfg.data.get('time_of_day'),\n",
    "        local_dataset_dir=cfg.data.get('local_dataset_dir'),\n",
    "        local_dataset_type=cfg.data.get('local_dataset_type', 'COCODetectionDataset'),\n",
    "    )\n",
    "\n",
    "    print('Preparing val split export...')\n",
    "    prepare_zoo_split_export(\n",
    "        zoo_name=cfg.data['zoo_name'],\n",
    "        split=cfg.data['split_val'],\n",
    "        dataset_name=f\"{cfg.run_name}_{cfg.data['split_val']}\",\n",
    "        export_dir=str(REPO_ROOT / cfg.data['export_val_dir']),\n",
    "        manifest_path=str(REPO_ROOT / cfg.data['manifest_val']),\n",
    "        source=cfg.data.get('source', 'fiftyone_zoo'),\n",
    "        max_samples=cfg.data.get('max_samples_val'),\n",
    "        time_of_day=cfg.data.get('time_of_day'),\n",
    "        local_dataset_dir=cfg.data.get('local_dataset_dir'),\n",
    "        local_dataset_type=cfg.data.get('local_dataset_type', 'COCODetectionDataset'),\n",
    "    )\n",
    "else:\n",
    "    print('PREPARE_DATA=False -> expecting existing manifests')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e521f13455ca07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:30:22.790065Z",
     "start_time": "2026-02-09T12:30:21.932422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 5000 instances: 24279\n",
      "Val images: 1000 instances: 5130\n",
      "Train batches: 625 Val batches: 125\n"
     ]
    }
   ],
   "source": [
    "from pipelines.coco_dataset import build_dataloader\n",
    "from pipelines.contracts import DatasetManifest\n",
    "\n",
    "train_manifest = DatasetManifest.from_json(REPO_ROOT / cfg.data['manifest_train'])\n",
    "val_manifest = DatasetManifest.from_json(REPO_ROOT / cfg.data['manifest_val'])\n",
    "\n",
    "train_loader = build_dataloader(\n",
    "    train_manifest,\n",
    "    image_size=cfg.train.image_size,\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    num_workers=cfg.train.num_workers,\n",
    "    shuffle=True,\n",
    "    max_samples=cfg.data.get('max_samples_train'),\n",
    ")\n",
    "val_loader = build_dataloader(\n",
    "    val_manifest,\n",
    "    image_size=cfg.train.image_size,\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    num_workers=cfg.train.num_workers,\n",
    "    shuffle=False,\n",
    "    max_samples=cfg.data.get('max_samples_val'),\n",
    ")\n",
    "\n",
    "print('Train images:', train_manifest.num_images, 'instances:', train_manifest.num_instances)\n",
    "print('Val images:', val_manifest.num_images, 'instances:', val_manifest.num_instances)\n",
    "print('Train batches:', len(train_loader), 'Val batches:', len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e18a24d4e785af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: head.weight, head.bias\n",
      "\n",
      "Loaded pretrained backbone tensors from /tmp/mamba_vision_T2.pth.tar: 8\n",
      "Missing keys: ['patch_embed.conv_down.0.weight', 'patch_embed.conv_down.1.weight', 'patch_embed.conv_down.1.bias', 'patch_embed.conv_down.1.running_mean', 'patch_embed.conv_down.1.running_var', 'patch_embed.conv_down.3.weight', 'patch_embed.conv_down.4.weight', 'patch_embed.conv_down.4.bias', 'patch_embed.conv_down.4.running_mean', 'patch_embed.conv_down.4.running_var', 'levels.0.blocks.0.conv1.weight', 'levels.0.blocks.0.conv1.bias', 'levels.0.blocks.0.norm1.weight', 'levels.0.blocks.0.norm1.bias', 'levels.0.blocks.0.norm1.running_mean', 'levels.0.blocks.0.norm1.running_var', 'levels.0.blocks.0.conv2.weight', 'levels.0.blocks.0.conv2.bias', 'levels.0.blocks.0.norm2.weight', 'levels.0.blocks.0.norm2.bias', 'levels.0.blocks.0.norm2.running_mean', 'levels.0.blocks.0.norm2.running_var', 'levels.0.downsample.reduction.0.weight', 'levels.1.blocks.0.conv1.weight', 'levels.1.blocks.0.conv1.bias', 'levels.1.blocks.0.norm1.weight', 'levels.1.blocks.0.norm1.bias', 'levels.1.blocks.0.norm1.running_mean', 'levels.1.blocks.0.norm1.running_var', 'levels.1.blocks.0.conv2.weight', 'levels.1.blocks.0.conv2.bias', 'levels.1.blocks.0.norm2.weight', 'levels.1.blocks.0.norm2.bias', 'levels.1.blocks.0.norm2.running_mean', 'levels.1.blocks.0.norm2.running_var', 'levels.1.blocks.1.conv1.weight', 'levels.1.blocks.1.conv1.bias', 'levels.1.blocks.1.norm1.weight', 'levels.1.blocks.1.norm1.bias', 'levels.1.blocks.1.norm1.running_mean', 'levels.1.blocks.1.norm1.running_var', 'levels.1.blocks.1.conv2.weight', 'levels.1.blocks.1.conv2.bias', 'levels.1.blocks.1.norm2.weight', 'levels.1.blocks.1.norm2.bias', 'levels.1.blocks.1.norm2.running_mean', 'levels.1.blocks.1.norm2.running_var', 'levels.1.blocks.2.conv1.weight', 'levels.1.blocks.2.conv1.bias', 'levels.1.blocks.2.norm1.weight', 'levels.1.blocks.2.norm1.bias', 'levels.1.blocks.2.norm1.running_mean', 'levels.1.blocks.2.norm1.running_var', 'levels.1.blocks.2.conv2.weight', 'levels.1.blocks.2.conv2.bias', 'levels.1.blocks.2.norm2.weight', 'levels.1.blocks.2.norm2.bias', 'levels.1.blocks.2.norm2.running_mean', 'levels.1.blocks.2.norm2.running_var', 'levels.1.downsample.reduction.0.weight', 'levels.2.blocks.0.norm1.weight', 'levels.2.blocks.0.norm1.bias', 'levels.2.blocks.0.mixer.A_log', 'levels.2.blocks.0.mixer.D', 'levels.2.blocks.0.mixer.in_proj.weight', 'levels.2.blocks.0.mixer.x_proj.weight', 'levels.2.blocks.0.mixer.dt_proj.weight', 'levels.2.blocks.0.mixer.dt_proj.bias', 'levels.2.blocks.0.mixer.out_proj.weight', 'levels.2.blocks.0.mixer.conv1d_x.weight', 'levels.2.blocks.0.mixer.conv1d_z.weight', 'levels.2.blocks.0.norm2.weight', 'levels.2.blocks.0.norm2.bias', 'levels.2.blocks.0.mlp.fc1.weight', 'levels.2.blocks.0.mlp.fc1.bias', 'levels.2.blocks.0.mlp.fc2.weight', 'levels.2.blocks.0.mlp.fc2.bias', 'levels.2.blocks.1.norm1.weight', 'levels.2.blocks.1.norm1.bias', 'levels.2.blocks.1.mixer.A_log', 'levels.2.blocks.1.mixer.D', 'levels.2.blocks.1.mixer.in_proj.weight', 'levels.2.blocks.1.mixer.x_proj.weight', 'levels.2.blocks.1.mixer.dt_proj.weight', 'levels.2.blocks.1.mixer.dt_proj.bias', 'levels.2.blocks.1.mixer.out_proj.weight', 'levels.2.blocks.1.mixer.conv1d_x.weight', 'levels.2.blocks.1.mixer.conv1d_z.weight', 'levels.2.blocks.1.norm2.weight', 'levels.2.blocks.1.norm2.bias', 'levels.2.blocks.1.mlp.fc1.weight', 'levels.2.blocks.1.mlp.fc1.bias', 'levels.2.blocks.1.mlp.fc2.weight', 'levels.2.blocks.1.mlp.fc2.bias', 'levels.2.blocks.2.norm1.weight', 'levels.2.blocks.2.norm1.bias', 'levels.2.blocks.2.mixer.A_log', 'levels.2.blocks.2.mixer.D', 'levels.2.blocks.2.mixer.in_proj.weight', 'levels.2.blocks.2.mixer.x_proj.weight', 'levels.2.blocks.2.mixer.dt_proj.weight', 'levels.2.blocks.2.mixer.dt_proj.bias', 'levels.2.blocks.2.mixer.out_proj.weight', 'levels.2.blocks.2.mixer.conv1d_x.weight', 'levels.2.blocks.2.mixer.conv1d_z.weight', 'levels.2.blocks.2.norm2.weight', 'levels.2.blocks.2.norm2.bias', 'levels.2.blocks.2.mlp.fc1.weight', 'levels.2.blocks.2.mlp.fc1.bias', 'levels.2.blocks.2.mlp.fc2.weight', 'levels.2.blocks.2.mlp.fc2.bias', 'levels.2.blocks.3.norm1.weight', 'levels.2.blocks.3.norm1.bias', 'levels.2.blocks.3.mixer.A_log', 'levels.2.blocks.3.mixer.D', 'levels.2.blocks.3.mixer.in_proj.weight', 'levels.2.blocks.3.mixer.x_proj.weight', 'levels.2.blocks.3.mixer.dt_proj.weight', 'levels.2.blocks.3.mixer.dt_proj.bias', 'levels.2.blocks.3.mixer.out_proj.weight', 'levels.2.blocks.3.mixer.conv1d_x.weight', 'levels.2.blocks.3.mixer.conv1d_z.weight', 'levels.2.blocks.3.norm2.weight', 'levels.2.blocks.3.norm2.bias', 'levels.2.blocks.3.mlp.fc1.weight', 'levels.2.blocks.3.mlp.fc1.bias', 'levels.2.blocks.3.mlp.fc2.weight', 'levels.2.blocks.3.mlp.fc2.bias', 'levels.2.blocks.4.norm1.weight', 'levels.2.blocks.4.norm1.bias', 'levels.2.blocks.4.mixer.A_log', 'levels.2.blocks.4.mixer.D', 'levels.2.blocks.4.mixer.in_proj.weight', 'levels.2.blocks.4.mixer.x_proj.weight', 'levels.2.blocks.4.mixer.dt_proj.weight', 'levels.2.blocks.4.mixer.dt_proj.bias', 'levels.2.blocks.4.mixer.out_proj.weight', 'levels.2.blocks.4.mixer.conv1d_x.weight', 'levels.2.blocks.4.mixer.conv1d_z.weight', 'levels.2.blocks.4.norm2.weight', 'levels.2.blocks.4.norm2.bias', 'levels.2.blocks.4.mlp.fc1.weight', 'levels.2.blocks.4.mlp.fc1.bias', 'levels.2.blocks.4.mlp.fc2.weight', 'levels.2.blocks.4.mlp.fc2.bias', 'levels.2.blocks.5.norm1.weight', 'levels.2.blocks.5.norm1.bias', 'levels.2.blocks.5.mixer.A_log', 'levels.2.blocks.5.mixer.D', 'levels.2.blocks.5.mixer.in_proj.weight', 'levels.2.blocks.5.mixer.x_proj.weight', 'levels.2.blocks.5.mixer.dt_proj.weight', 'levels.2.blocks.5.mixer.dt_proj.bias', 'levels.2.blocks.5.mixer.out_proj.weight', 'levels.2.blocks.5.mixer.conv1d_x.weight', 'levels.2.blocks.5.mixer.conv1d_z.weight', 'levels.2.blocks.5.norm2.weight', 'levels.2.blocks.5.norm2.bias', 'levels.2.blocks.5.mlp.fc1.weight', 'levels.2.blocks.5.mlp.fc1.bias', 'levels.2.blocks.5.mlp.fc2.weight', 'levels.2.blocks.5.mlp.fc2.bias', 'levels.2.blocks.6.norm1.weight', 'levels.2.blocks.6.norm1.bias', 'levels.2.blocks.6.mixer.qkv.weight', 'levels.2.blocks.6.mixer.qkv.bias', 'levels.2.blocks.6.mixer.proj.weight', 'levels.2.blocks.6.mixer.proj.bias', 'levels.2.blocks.6.norm2.weight', 'levels.2.blocks.6.norm2.bias', 'levels.2.blocks.6.mlp.fc1.weight', 'levels.2.blocks.6.mlp.fc1.bias', 'levels.2.blocks.6.mlp.fc2.weight', 'levels.2.blocks.6.mlp.fc2.bias', 'levels.2.blocks.7.norm1.weight', 'levels.2.blocks.7.norm1.bias', 'levels.2.blocks.7.mixer.qkv.weight', 'levels.2.blocks.7.mixer.qkv.bias', 'levels.2.blocks.7.mixer.proj.weight', 'levels.2.blocks.7.mixer.proj.bias', 'levels.2.blocks.7.norm2.weight', 'levels.2.blocks.7.norm2.bias', 'levels.2.blocks.7.mlp.fc1.weight', 'levels.2.blocks.7.mlp.fc1.bias', 'levels.2.blocks.7.mlp.fc2.weight', 'levels.2.blocks.7.mlp.fc2.bias', 'levels.2.blocks.8.norm1.weight', 'levels.2.blocks.8.norm1.bias', 'levels.2.blocks.8.mixer.qkv.weight', 'levels.2.blocks.8.mixer.qkv.bias', 'levels.2.blocks.8.mixer.proj.weight', 'levels.2.blocks.8.mixer.proj.bias', 'levels.2.blocks.8.norm2.weight', 'levels.2.blocks.8.norm2.bias', 'levels.2.blocks.8.mlp.fc1.weight', 'levels.2.blocks.8.mlp.fc1.bias', 'levels.2.blocks.8.mlp.fc2.weight', 'levels.2.blocks.8.mlp.fc2.bias', 'levels.2.blocks.9.norm1.weight', 'levels.2.blocks.9.norm1.bias', 'levels.2.blocks.9.mixer.qkv.weight', 'levels.2.blocks.9.mixer.qkv.bias', 'levels.2.blocks.9.mixer.proj.weight', 'levels.2.blocks.9.mixer.proj.bias', 'levels.2.blocks.9.norm2.weight', 'levels.2.blocks.9.norm2.bias', 'levels.2.blocks.9.mlp.fc1.weight', 'levels.2.blocks.9.mlp.fc1.bias', 'levels.2.blocks.9.mlp.fc2.weight', 'levels.2.blocks.9.mlp.fc2.bias', 'levels.2.blocks.10.norm1.weight', 'levels.2.blocks.10.norm1.bias', 'levels.2.blocks.10.mixer.qkv.weight', 'levels.2.blocks.10.mixer.qkv.bias', 'levels.2.blocks.10.mixer.proj.weight', 'levels.2.blocks.10.mixer.proj.bias', 'levels.2.blocks.10.norm2.weight', 'levels.2.blocks.10.norm2.bias', 'levels.2.blocks.10.mlp.fc1.weight', 'levels.2.blocks.10.mlp.fc1.bias', 'levels.2.blocks.10.mlp.fc2.weight', 'levels.2.blocks.10.mlp.fc2.bias', 'levels.2.downsample.reduction.0.weight', 'levels.3.blocks.0.norm1.weight', 'levels.3.blocks.0.norm1.bias', 'levels.3.blocks.0.mixer.A_log', 'levels.3.blocks.0.mixer.D', 'levels.3.blocks.0.mixer.in_proj.weight', 'levels.3.blocks.0.mixer.x_proj.weight', 'levels.3.blocks.0.mixer.dt_proj.weight', 'levels.3.blocks.0.mixer.dt_proj.bias', 'levels.3.blocks.0.mixer.out_proj.weight', 'levels.3.blocks.0.mixer.conv1d_x.weight', 'levels.3.blocks.0.mixer.conv1d_z.weight', 'levels.3.blocks.0.norm2.weight', 'levels.3.blocks.0.norm2.bias', 'levels.3.blocks.0.mlp.fc1.weight', 'levels.3.blocks.0.mlp.fc1.bias', 'levels.3.blocks.0.mlp.fc2.weight', 'levels.3.blocks.0.mlp.fc2.bias', 'levels.3.blocks.1.norm1.weight', 'levels.3.blocks.1.norm1.bias', 'levels.3.blocks.1.mixer.A_log', 'levels.3.blocks.1.mixer.D', 'levels.3.blocks.1.mixer.in_proj.weight', 'levels.3.blocks.1.mixer.x_proj.weight', 'levels.3.blocks.1.mixer.dt_proj.weight', 'levels.3.blocks.1.mixer.dt_proj.bias', 'levels.3.blocks.1.mixer.out_proj.weight', 'levels.3.blocks.1.mixer.conv1d_x.weight', 'levels.3.blocks.1.mixer.conv1d_z.weight', 'levels.3.blocks.1.norm2.weight', 'levels.3.blocks.1.norm2.bias', 'levels.3.blocks.1.mlp.fc1.weight', 'levels.3.blocks.1.mlp.fc1.bias', 'levels.3.blocks.1.mlp.fc2.weight', 'levels.3.blocks.1.mlp.fc2.bias', 'levels.3.blocks.2.norm1.weight', 'levels.3.blocks.2.norm1.bias', 'levels.3.blocks.2.mixer.qkv.weight', 'levels.3.blocks.2.mixer.qkv.bias', 'levels.3.blocks.2.mixer.proj.weight', 'levels.3.blocks.2.mixer.proj.bias', 'levels.3.blocks.2.norm2.weight', 'levels.3.blocks.2.norm2.bias', 'levels.3.blocks.2.mlp.fc1.weight', 'levels.3.blocks.2.mlp.fc1.bias', 'levels.3.blocks.2.mlp.fc2.weight', 'levels.3.blocks.2.mlp.fc2.bias', 'levels.3.blocks.3.norm1.weight', 'levels.3.blocks.3.norm1.bias', 'levels.3.blocks.3.mixer.qkv.weight', 'levels.3.blocks.3.mixer.qkv.bias', 'levels.3.blocks.3.mixer.proj.weight', 'levels.3.blocks.3.mixer.proj.bias', 'levels.3.blocks.3.norm2.weight', 'levels.3.blocks.3.norm2.bias', 'levels.3.blocks.3.mlp.fc1.weight', 'levels.3.blocks.3.mlp.fc1.bias', 'levels.3.blocks.3.mlp.fc2.weight', 'levels.3.blocks.3.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'norm.running_mean', 'norm.running_var']\n",
      "Unexpected keys: ['epoch', 'arch', 'state_dict', 'optimizer', 'version', 'args', 'amp_scaler', 'metric']\n",
      "{'trainable': 43573319, 'frozen': 0, 'total': 43573319}\n"
     ]
    }
   ],
   "source": [
    "from pipelines.lora import (\n",
    "    collect_trainable_parameter_summary,\n",
    "    configure_lora_training,\n",
    "    inject_lora_modules,\n",
    ")\n",
    "from pipelines.model_loader import create_model_from_config\n",
    "from pipelines.training import load_checkpoint, resolve_device\n",
    "\n",
    "\n",
    "device = resolve_device(cfg.train.device)\n",
    "model = create_model_from_config(cfg.model, device=str(device))\n",
    "\n",
    "if cfg.model.base_checkpoint:\n",
    "    load_checkpoint(REPO_ROOT / cfg.model.base_checkpoint, model)\n",
    "    print('Loaded base checkpoint:', REPO_ROOT / cfg.model.base_checkpoint)\n",
    "\n",
    "if cfg.lora is not None:\n",
    "    replaced_layers = inject_lora_modules(\n",
    "        model.backbone,\n",
    "        rank=cfg.lora.rank,\n",
    "        alpha=cfg.lora.alpha,\n",
    "        dropout=cfg.lora.dropout,\n",
    "        target_rule=cfg.lora.target_rule,\n",
    "    )\n",
    "    configure_lora_training(model, freeze_neck=cfg.freeze.neck, freeze_head=cfg.freeze.head)\n",
    "    print('LoRA layers injected:', len(replaced_layers))\n",
    "\n",
    "summary = collect_trainable_parameter_summary(model)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e5e6d8ff60815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale outputs: [(8, 13, 80, 80), (8, 13, 40, 40), (8, 13, 20, 20)]\n",
      "Sanity loss: {'loss': 4.443694114685059, 'obj_loss': 1.8640356063842773, 'box_loss': 0.08401885628700256, 'cls_loss': 2.1595640182495117}\n"
     ]
    }
   ],
   "source": [
    "from pipelines.yolo_ops import MultiScaleYoloLoss\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = MultiScaleYoloLoss(num_classes=cfg.model.num_classes).to(device)\n",
    "\n",
    "images, targets = next(iter(train_loader))\n",
    "images = images.to(device)\n",
    "targets = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "\n",
    "losses = criterion(outputs, targets)\n",
    "print('Scale outputs:', [tuple(o.shape) for o in outputs])\n",
    "print('Sanity loss:', {k: float(v.detach().cpu()) for k, v in losses.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faeb226be2de3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.21 GB\n",
      "GPU memory reserved : 2.02 GB\n"
     ]
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)\n",
    "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)\n",
    "    print(f'GPU memory allocated: {allocated:.2f} GB')\n",
    "    print(f'GPU memory reserved : {reserved:.2f} GB')\n",
    "else:\n",
    "    print('Non-CUDA device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8013fed8d98e5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/pipelines/training.py:91: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and precision.lower() == \"fp16\"))\n",
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot train loss: 4.309391689300537\n",
      "Pilot val loss  : 4.569975137710571\n"
     ]
    }
   ],
   "source": [
    "from pipelines.training import fit_model\n",
    "\n",
    "pilot_history = fit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    train_cfg=cfg.train,\n",
    "    ckpt_cfg=cfg.ckpt,\n",
    "    num_classes=cfg.model.num_classes,\n",
    "    run_mode='pilot',\n",
    ")\n",
    "\n",
    "print('Pilot train loss:', pilot_history['train'][-1].loss)\n",
    "print('Pilot val loss  :', pilot_history['val'][-1].loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a009e32b0140bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_MODE is pilot -> full run step is skipped\n"
     ]
    }
   ],
   "source": [
    "if RUN_MODE == 'full':\n",
    "    if AUTO_CONFIRM_FULL:\n",
    "        confirm = 'yes'\n",
    "    else:\n",
    "        confirm = input('Pilot complete. Start FULL training? (yes/no): ').strip().lower()\n",
    "    if confirm != 'yes':\n",
    "        raise RuntimeError('Cancelled by user before full training')\n",
    "else:\n",
    "    print('RUN_MODE is pilot -> full run step is skipped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24f4d66841c0b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot-only run completed\n"
     ]
    }
   ],
   "source": [
    "full_history = None\n",
    "if RUN_MODE == 'full':\n",
    "    full_history = fit_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        train_cfg=cfg.train,\n",
    "        ckpt_cfg=cfg.ckpt,\n",
    "        num_classes=cfg.model.num_classes,\n",
    "        run_mode='full',\n",
    "    )\n",
    "    print('Full train loss:', full_history['train'][-1].loss)\n",
    "    print('Full val loss  :', full_history['val'][-1].loss)\n",
    "else:\n",
    "    print('Pilot-only run completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff18135f4bdf0c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train checkpoint path: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/base/coco_base.ckpt\n"
     ]
    }
   ],
   "source": [
    "from pipelines.lora import save_lora_adapters\n",
    "\n",
    "if cfg.lora is not None:\n",
    "    lora_path = REPO_ROOT / cfg.data['lora_output_path']\n",
    "    lora_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_lora_adapters(\n",
    "        model,\n",
    "        output_path=str(lora_path),\n",
    "        metadata={\n",
    "            'run_name': cfg.run_name,\n",
    "            'base_checkpoint': cfg.model.base_checkpoint,\n",
    "        },\n",
    "    )\n",
    "    print('Saved LoRA adapters:', lora_path)\n",
    "\n",
    "print('Train checkpoint path:', REPO_ROOT / cfg.ckpt.output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b69a42ffedec3",
   "metadata": {},
   "source": [
    "## Done checklist\n",
    "\n",
    "- [ ] Dependencies verified\n",
    "- [ ] Manifests found and non-empty\n",
    "- [ ] Shape/loss sanity passed\n",
    "- [ ] Pilot run completed\n",
    "- [ ] Full run confirmed (if needed)\n",
    "- [ ] Checkpoint/adapter saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6aa9c8acb82d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
