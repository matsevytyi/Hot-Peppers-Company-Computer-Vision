{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train LoRA BDD Night (Mamba-Vision)\n",
        "\n",
        "Notebook-first pipeline with safety gates:\n",
        "\n",
        "- dependency checks\n",
        "- optional FiftyOne export + manifest generation\n",
        "- one-batch shape/loss sanity\n",
        "- mandatory pilot before full run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision\n",
            "Run mode: pilot\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path) -> Path:\n",
        "    for candidate in (start, *start.parents):\n",
        "        if (candidate / '.git').exists():\n",
        "            return candidate\n",
        "    return start\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root(Path.cwd().resolve())\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "RUN_MODE = 'pilot'  # pilot | full\n",
        "PREPARE_DATA = False\n",
        "AUTO_CONFIRM_FULL = False\n",
        "\n",
        "print('Repo root:', REPO_ROOT)\n",
        "print('Run mode:', RUN_MODE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/configs/training/lora_bdd_night.yaml')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CONFIG_PATH = REPO_ROOT / 'configs/training/lora_bdd_night.yaml'\n",
        "CONFIG_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'torch': True,\n",
              " 'torchvision': True,\n",
              " 'yaml': True,\n",
              " 'safetensors': True,\n",
              " 'tqdm': True,\n",
              " 'einops': True,\n",
              " 'fiftyone': True,\n",
              " 'mambavision': True,\n",
              " 'wandb': True}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pipelines.dependencies import check_packages, assert_required_packages\n",
        "\n",
        "required = ['torch', 'torchvision', 'yaml', 'safetensors', 'tqdm', 'einops']\n",
        "optional = ['fiftyone', 'mambavision', 'wandb']\n",
        "status = check_packages(required + optional)\n",
        "status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Core dependencies look good\n"
          ]
        }
      ],
      "source": [
        "assert_required_packages(['torch', 'torchvision', 'yaml', 'safetensors', 'einops'])\n",
        "print('Core dependencies look good')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainConfig(run_name='lora_bdd_night', data={'source': 'fiftyone_zoo', 'zoo_name': 'bdd100k', 'split_train': 'train', 'split_val': 'validation', 'time_of_day': ['night'], 'manifest_train': 'configs/manifests/bdd_night_train.json', 'manifest_val': 'configs/manifests/bdd_night_val.json', 'export_train_dir': 'data/exports/bdd_night/train', 'export_val_dir': 'data/exports/bdd_night/val', 'max_samples_train': None, 'max_samples_val': None, 'local_dataset_dir': '/teamspace/studios/this_studio/datasets/bdd100k:-images-100k', 'local_dataset_type': 'COCODetectionDataset', 'lora_output_path': '/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/lora/bdd_night.safetensors'}, model=ModelSection(backbone='mamba_vision_T2', num_classes=8, pretrained=False, checkpoint_path='', base_checkpoint='/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/base/coco_base.ckpt', model_file='/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/mamba-vision-ours/model.py'), train=TrainSection(epochs=12, batch_size=8, num_workers=4, lr=0.0002, weight_decay=0.0001, scheduler='cosine', pilot_steps=5, pilot_val_steps=2, precision='fp16', device='cuda', image_size=640, grad_clip_norm=1.0), ckpt=CkptConfig(output_path='/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/lora/bdd_night_train_state.ckpt', save_top_k=3, save_last=True), lora=LoRAConfig(rank=8, alpha=16, dropout=0.05, target_rule='all_linear_except_head'), freeze=FreezeConfig(backbone_base=True, neck=True, head=True))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pipelines.contracts import TrainConfig\n",
        "\n",
        "cfg = TrainConfig.from_yaml(CONFIG_PATH)\n",
        "cfg.model.model_file = str((REPO_ROOT / cfg.model.model_file).resolve())\n",
        "cfg.ckpt.output_path = str((REPO_ROOT / cfg.ckpt.output_path).resolve())\n",
        "if cfg.model.base_checkpoint:\n",
        "    cfg.model.base_checkpoint = str((REPO_ROOT / cfg.model.base_checkpoint).resolve())\n",
        "if cfg.data.get('lora_output_path'):\n",
        "    cfg.data['lora_output_path'] = str((REPO_ROOT / cfg.data['lora_output_path']).resolve())\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PREPARE_DATA=False -> expecting existing manifests\n"
          ]
        }
      ],
      "source": [
        "from pipelines.fiftyone_data import prepare_zoo_split_export\n",
        "\n",
        "if PREPARE_DATA:\n",
        "    print('Preparing train split export...')\n",
        "    prepare_zoo_split_export(\n",
        "        zoo_name=cfg.data['zoo_name'],\n",
        "        split=cfg.data['split_train'],\n",
        "        dataset_name=f\"{cfg.run_name}_{cfg.data['split_train']}\",\n",
        "        export_dir=str(REPO_ROOT / cfg.data['export_train_dir']),\n",
        "        manifest_path=str(REPO_ROOT / cfg.data['manifest_train']),\n",
        "        source=cfg.data.get('source', 'fiftyone_zoo'),\n",
        "        max_samples=cfg.data.get('max_samples_train'),\n",
        "        time_of_day=cfg.data.get('time_of_day'),\n",
        "        local_dataset_dir=cfg.data.get('local_dataset_dir'),\n",
        "        local_dataset_type=cfg.data.get('local_dataset_type', 'COCODetectionDataset'),\n",
        "    )\n",
        "\n",
        "    print('Preparing val split export...')\n",
        "    prepare_zoo_split_export(\n",
        "        zoo_name=cfg.data['zoo_name'],\n",
        "        split=cfg.data['split_val'],\n",
        "        dataset_name=f\"{cfg.run_name}_{cfg.data['split_val']}\",\n",
        "        export_dir=str(REPO_ROOT / cfg.data['export_val_dir']),\n",
        "        manifest_path=str(REPO_ROOT / cfg.data['manifest_val']),\n",
        "        source=cfg.data.get('source', 'fiftyone_zoo'),\n",
        "        max_samples=cfg.data.get('max_samples_val'),\n",
        "        time_of_day=cfg.data.get('time_of_day'),\n",
        "        local_dataset_dir=cfg.data.get('local_dataset_dir'),\n",
        "        local_dataset_type=cfg.data.get('local_dataset_type', 'COCODetectionDataset'),\n",
        "    )\n",
        "else:\n",
        "    print('PREPARE_DATA=False -> expecting existing manifests')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 27971 instances: 367425\n",
            "Val images: 3929 instances: 51889\n",
            "Train batches: 3497 Val batches: 492\n"
          ]
        }
      ],
      "source": [
        "from pipelines.coco_dataset import build_dataloader\n",
        "from pipelines.contracts import DatasetManifest\n",
        "\n",
        "train_manifest = DatasetManifest.from_json(REPO_ROOT / cfg.data['manifest_train'])\n",
        "val_manifest = DatasetManifest.from_json(REPO_ROOT / cfg.data['manifest_val'])\n",
        "\n",
        "train_loader = build_dataloader(\n",
        "    train_manifest,\n",
        "    image_size=cfg.train.image_size,\n",
        "    batch_size=cfg.train.batch_size,\n",
        "    num_workers=cfg.train.num_workers,\n",
        "    shuffle=True,\n",
        "    max_samples=cfg.data.get('max_samples_train'),\n",
        ")\n",
        "val_loader = build_dataloader(\n",
        "    val_manifest,\n",
        "    image_size=cfg.train.image_size,\n",
        "    batch_size=cfg.train.batch_size,\n",
        "    num_workers=cfg.train.num_workers,\n",
        "    shuffle=False,\n",
        "    max_samples=cfg.data.get('max_samples_val'),\n",
        ")\n",
        "\n",
        "print('Train images:', train_manifest.num_images, 'instances:', train_manifest.num_instances)\n",
        "print('Val images:', val_manifest.num_images, 'instances:', val_manifest.num_instances)\n",
        "print('Train batches:', len(train_loader), 'Val batches:', len(val_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded base checkpoint: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/base/coco_base.ckpt\n",
            "LoRA layers injected: 76\n",
            "{'trainable': 756864, 'frozen': 43573319, 'total': 44330183}\n"
          ]
        }
      ],
      "source": [
        "from pipelines.lora import (\n",
        "    collect_trainable_parameter_summary,\n",
        "    configure_lora_training,\n",
        "    inject_lora_modules,\n",
        ")\n",
        "from pipelines.model_loader import create_model_from_config\n",
        "from pipelines.training import load_checkpoint, resolve_device\n",
        "\n",
        "\n",
        "device = resolve_device(cfg.train.device)\n",
        "model = create_model_from_config(cfg.model, device=str(device))\n",
        "\n",
        "if cfg.model.base_checkpoint:\n",
        "    load_checkpoint(REPO_ROOT / cfg.model.base_checkpoint, model)\n",
        "    print('Loaded base checkpoint:', REPO_ROOT / cfg.model.base_checkpoint)\n",
        "\n",
        "if cfg.lora is not None:\n",
        "    replaced_layers = inject_lora_modules(\n",
        "        model.backbone,\n",
        "        rank=cfg.lora.rank,\n",
        "        alpha=cfg.lora.alpha,\n",
        "        dropout=cfg.lora.dropout,\n",
        "        target_rule=cfg.lora.target_rule,\n",
        "    )\n",
        "    configure_lora_training(model, freeze_neck=cfg.freeze.neck, freeze_head=cfg.freeze.head)\n",
        "    print('LoRA layers injected:', len(replaced_layers))\n",
        "\n",
        "summary = collect_trainable_parameter_summary(model)\n",
        "print(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scale outputs: [(8, 13, 80, 80), (8, 13, 40, 40), (8, 13, 20, 20)]\n",
            "Sanity loss: {'loss': 3.3725879192352295, 'obj_loss': 1.8322288990020752, 'box_loss': 0.05520438402891159, 'cls_loss': 1.2643370628356934}\n"
          ]
        }
      ],
      "source": [
        "from pipelines.yolo_ops import MultiScaleYoloLoss\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = MultiScaleYoloLoss(num_classes=cfg.model.num_classes).to(device)\n",
        "\n",
        "images, targets = next(iter(train_loader))\n",
        "images = images.to(device)\n",
        "targets = [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "\n",
        "losses = criterion(outputs, targets)\n",
        "print('Scale outputs:', [tuple(o.shape) for o in outputs])\n",
        "print('Sanity loss:', {k: float(v.detach().cpu()) for k, v in losses.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory allocated: 0.22 GB\n",
            "GPU memory reserved : 1.99 GB\n"
          ]
        }
      ],
      "source": [
        "if device.type == 'cuda':\n",
        "    allocated = torch.cuda.memory_allocated(device) / (1024 ** 3)\n",
        "    reserved = torch.cuda.memory_reserved(device) / (1024 ** 3)\n",
        "    print(f'GPU memory allocated: {allocated:.2f} GB')\n",
        "    print(f'GPU memory reserved : {reserved:.2f} GB')\n",
        "else:\n",
        "    print('Non-CUDA device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/pipelines/training.py:91: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\" and precision.lower() == \"fp16\"))\n",
            "                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pilot train loss: 4.141508388519287\n",
            "Pilot val loss  : 3.735966205596924\n"
          ]
        }
      ],
      "source": [
        "from pipelines.training import fit_model\n",
        "\n",
        "pilot_history = fit_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    train_cfg=cfg.train,\n",
        "    ckpt_cfg=cfg.ckpt,\n",
        "    num_classes=cfg.model.num_classes,\n",
        "    run_mode='pilot',\n",
        ")\n",
        "\n",
        "print('Pilot train loss:', pilot_history['train'][-1].loss)\n",
        "print('Pilot val loss  :', pilot_history['val'][-1].loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN_MODE is pilot -> full run step is skipped\n"
          ]
        }
      ],
      "source": [
        "if RUN_MODE == 'full':\n",
        "    if AUTO_CONFIRM_FULL:\n",
        "        confirm = 'yes'\n",
        "    else:\n",
        "        confirm = input('Pilot complete. Start FULL training? (yes/no): ').strip().lower()\n",
        "    if confirm != 'yes':\n",
        "        raise RuntimeError('Cancelled by user before full training')\n",
        "else:\n",
        "    print('RUN_MODE is pilot -> full run step is skipped')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pilot-only run completed\n"
          ]
        }
      ],
      "source": [
        "full_history = None\n",
        "if RUN_MODE == 'full':\n",
        "    full_history = fit_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        train_cfg=cfg.train,\n",
        "        ckpt_cfg=cfg.ckpt,\n",
        "        num_classes=cfg.model.num_classes,\n",
        "        run_mode='full',\n",
        "    )\n",
        "    print('Full train loss:', full_history['train'][-1].loss)\n",
        "    print('Full val loss  :', full_history['val'][-1].loss)\n",
        "else:\n",
        "    print('Pilot-only run completed')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved LoRA adapters: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/lora/bdd_night.safetensors\n",
            "Train checkpoint path: /teamspace/studios/this_studio/Hot-Peppers-Company-Computer-Vision/checkpoints/lora/bdd_night_train_state.ckpt\n"
          ]
        }
      ],
      "source": [
        "from pipelines.lora import save_lora_adapters\n",
        "\n",
        "if cfg.lora is not None:\n",
        "    lora_path = REPO_ROOT / cfg.data['lora_output_path']\n",
        "    lora_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    save_lora_adapters(\n",
        "        model,\n",
        "        output_path=str(lora_path),\n",
        "        metadata={\n",
        "            'run_name': cfg.run_name,\n",
        "            'base_checkpoint': cfg.model.base_checkpoint,\n",
        "        },\n",
        "    )\n",
        "    print('Saved LoRA adapters:', lora_path)\n",
        "\n",
        "print('Train checkpoint path:', REPO_ROOT / cfg.ckpt.output_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724e0a71",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
