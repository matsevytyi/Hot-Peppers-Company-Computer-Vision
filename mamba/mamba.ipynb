{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Mamba UAV Detector - Incremental Training Notebook\n",
                "\n",
                "This notebook implements an **incremental training pipeline** for the Mamba-based UAV detector.\n",
                "\n",
                "## Storage-Aware Workflow\n",
                "Due to storage constraints (~10GB available), we train one UAV type at a time:\n",
                "```\n",
                "Download UAV-A ‚Üí Train ‚Üí Save checkpoint ‚Üí Delete UAV-A\n",
                "Download UAV-B ‚Üí Load checkpoint ‚Üí Continue training ‚Üí Delete UAV-B\n",
                "... repeat for all 12 UAV types ...\n",
                "```\n",
                "\n",
                "## Table of Contents\n",
                "1. [Setup & Imports](#1-setup)\n",
                "2. [Download Utilities](#2-download)\n",
                "3. [Configuration](#3-config)\n",
                "4. [Model Setup](#4-model)\n",
                "5. [Training Loop (per UAV part)](#5-training)\n",
                "6. [Cleanup](#6-cleanup)\n",
                "7. [Evaluation](#7-evaluation)\n",
                "8. [Export Model](#8-export)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='1-setup'></a>\n",
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üçé Using Apple MPS (with LSTM fallback for Mamba)\n",
                        "\n",
                        "üì¶ Available UAV types for download: ['A', 'B', 'C', 'D', 'E', 'F', 'JSON']\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import warnings\n",
                "from pathlib import Path\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path(os.getcwd()).parent\n",
                "if str(PROJECT_ROOT) not in sys.path:\n",
                "    sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "# Core imports\n",
                "import torch\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
                "\n",
                "# Local imports\n",
                "from mamba.config import Config\n",
                "from mamba.trainer import MambaDetectorModule\n",
                "from mamba.dataset import (\n",
                "    download_uav_part,\n",
                "    cleanup_uav_part,\n",
                "    get_available_uav_types,\n",
                "    create_dataloaders,\n",
                "    DATASET_URLS,\n",
                ")\n",
                "\n",
                "# Device detection\n",
                "if torch.cuda.is_available():\n",
                "    DEVICE = 'cuda'\n",
                "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
                "elif torch.backends.mps.is_available():\n",
                "    DEVICE = 'mps'\n",
                "    print(\"üçé Using Apple MPS (with LSTM fallback for Mamba)\")\n",
                "else:\n",
                "    DEVICE = 'cpu'\n",
                "    print(\"üíª Using CPU\")\n",
                "\n",
                "print(f\"\\nüì¶ Available UAV types for download: {list(DATASET_URLS.keys())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='2-download'></a>\n",
                "## 2. Download Utilities\n",
                "\n",
                "Functions to download, extract, and cleanup UAV parts one at a time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÅ Currently downloaded UAV types: ['A']\n",
                        "\n",
                        "üíæ Estimated storage per UAV type:\n",
                        "   - UAV-A to UAV-F: ~10GB each\n",
                        "   - Total dataset: ~100GB\n",
                        "   - With incremental training, you only need ~10GB at a time\n"
                    ]
                }
            ],
            "source": [
                "# Check what's currently downloaded\n",
                "DATA_ROOT = PROJECT_ROOT / \"data\" / \"MMFW-UAV\" / \"raw\"\n",
                "available = get_available_uav_types(str(DATA_ROOT))\n",
                "print(f\"üìÅ Currently downloaded UAV types: {available if available else 'None'}\")\n",
                "\n",
                "# Estimate storage per UAV type (~1-2GB each)\n",
                "print(\"\\nüíæ Estimated storage per UAV type:\")\n",
                "print(\"   - UAV-A to UAV-F: ~10GB each\")\n",
                "print(\"   - Total dataset: ~100GB\")\n",
                "print(\"   - With incremental training, you only need ~10GB at a time\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='3-config'></a>\n",
                "## 3. Configuration\n",
                "\n",
                "Training hyperparameters. Adjust based on your hardware."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Configuration loaded\n",
                        "   Checkpoint dir: /Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/outputs/checkpoints\n"
                    ]
                }
            ],
            "source": [
                "# Training configuration\n",
                "config = Config()\n",
                "\n",
                "# Data settings\n",
                "config.data.data_root = str(DATA_ROOT)\n",
                "config.data.batch_size = 4  # Reduce if OOM\n",
                "config.data.num_workers = 2  # 0 for debugging\n",
                "config.data.sequence_length = 10\n",
                "config.data.stride = 5\n",
                "config.data.img_size = 640\n",
                "config.data.sensor_type = \"Zoom\"  # Zoom, Wide, or Infrared\n",
                "config.data.view = \"Top_Down\"  # Top_Down, Horizontal, or Bottom_Up\n",
                "\n",
                "# Model settings\n",
                "config.model.backbone = \"mobilevit_s\"\n",
                "config.model.d_model = 256\n",
                "config.model.d_state = 16\n",
                "config.model.mamba_layers = 4\n",
                "\n",
                "# Training settings\n",
                "config.training.max_epochs = 10  # Epochs per UAV part\n",
                "config.training.lr = 1e-3\n",
                "config.training.weight_decay = 1e-4\n",
                "\n",
                "# Checkpoint path\n",
                "CHECKPOINT_DIR = PROJECT_ROOT / \"outputs\" / \"checkpoints\"\n",
                "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "LATEST_CHECKPOINT = CHECKPOINT_DIR / \"latest.ckpt\"\n",
                "\n",
                "print(f\"‚úÖ Configuration loaded\")\n",
                "print(f\"   Checkpoint dir: {CHECKPOINT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='4-model'></a>\n",
                "## 4. Model Setup\n",
                "\n",
                "Initialize model, optionally loading from checkpoint for continued training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üÜï Creating new model\n",
                        "‚úÖ New model created\n",
                        "\n",
                        "üìä Model stats:\n",
                        "   Total parameters: 7,250,597\n",
                        "   Trainable parameters: 7,250,597\n"
                    ]
                }
            ],
            "source": [
                "# Initialize or load model\n",
                "if LATEST_CHECKPOINT.exists():\n",
                "    print(f\"üìÇ Loading checkpoint: {LATEST_CHECKPOINT}\")\n",
                "    model = MambaDetectorModule.load_from_checkpoint(\n",
                "        str(LATEST_CHECKPOINT),\n",
                "        config=config,\n",
                "    )\n",
                "    print(\"‚úÖ Model loaded from checkpoint\")\n",
                "else:\n",
                "    print(\"üÜï Creating new model\")\n",
                "    model = MambaDetectorModule(config)\n",
                "    print(\"‚úÖ New model created\")\n",
                "\n",
                "# Print model summary\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"\\nüìä Model stats:\")\n",
                "print(f\"   Total parameters: {total_params:,}\")\n",
                "print(f\"   Trainable parameters: {trainable_params:,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='5-training'></a>\n",
                "## 5. Training Loop (per UAV part)\n",
                "\n",
                "**‚ö†Ô∏è Run this cell for each UAV part you want to train on:**\n",
                "1. Change `CURRENT_UAV` to the UAV type you want to download and train\n",
                "2. Run the cell - it will download, train, and save checkpoint\n",
                "3. Optionally run the cleanup cell to delete downloaded data\n",
                "4. Repeat with the next UAV type"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (mps), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "üéØ TRAINING ON UAV-A\n",
                        "============================================================\n",
                        "\n",
                        "üì• Step 1: Downloading data...\n",
                        "‚úÖ UAV-A already exists at: /Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/data/MMFW-UAV/raw/Fixed-wing-UAV-A\n",
                        "   Skipping download. Delete folder to re-download.\n",
                        "\n",
                        "üìÅ Step 2: Creating dataloaders for UAV-A...\n",
                        "   Train batches: 73\n",
                        "   Val batches: 73\n",
                        "\n",
                        "üèãÔ∏è Step 3: Setting up trainer...\n",
                        "\n",
                        "üöÄ Step 4: Training for 1 epochs...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  | Name      | Type             | Params | Mode  | FLOPs\n",
                        "---------------------------------------------------------------\n",
                        "0 | model     | MambaUAVDetector | 7.3 M  | train | 0    \n",
                        "1 | criterion | DetectionLoss    | 0      | train | 0    \n",
                        "---------------------------------------------------------------\n",
                        "7.3 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "7.3 M     Total params\n",
                        "29.002    Total estimated model params size (MB)\n",
                        "455       Modules in train mode\n",
                        "0         Modules in eval mode\n",
                        "0         Total Flops\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d3c63e9fd5d544b3b909e05e77b0df11",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>\n",
                        "  data = fetch_version_info()\n",
                        "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>\n",
                        "  data = fetch_version_info()\n",
                        "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
                        "  self._set_keys()\n",
                        "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
                        "  self._set_keys()\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "MPS backend out of memory (MPS allocated: 4.00 GiB, other allocations: 12.21 GiB, max allowed: 20.13 GiB). Tried to allocate 6.10 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Step 4: Train!\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Step 4: Training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS_THIS_PART\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Step 5: Save checkpoint for next part\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müíæ Step 5: Saving checkpoint...\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:584\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:630\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    623\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    624\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    626\u001b[39m     ckpt_path,\n\u001b[32m    627\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    628\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    629\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1079\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1076\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1078\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1084\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1121\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1120\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1123\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1150\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1147\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1149\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1152\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1154\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:146\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    145\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/evaluation_loop.py:441\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    435\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    436\u001b[39m step_args = (\n\u001b[32m    437\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    440\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/trainer.py:77\u001b[39m, in \u001b[36mMambaDetectorModule.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m     76\u001b[39m     images, targets = batch\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     targets_used = \u001b[38;5;28mself\u001b[39m._select_targets(predictions, targets)\n\u001b[32m     80\u001b[39m     loss_dict = \u001b[38;5;28mself\u001b[39m.criterion(predictions, targets_used)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/model.py:58\u001b[39m, in \u001b[36mMambaUAVDetector.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     55\u001b[39m bsz, seq_len, channels, height, width = x.shape\n\u001b[32m     56\u001b[39m x = x.reshape(bsz * seq_len, channels, height, width)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features.dim() == \u001b[32m4\u001b[39m:\n\u001b[32m     60\u001b[39m     features = \u001b[38;5;28mself\u001b[39m.pool(features).flatten(\u001b[32m1\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/backbone.py:19\u001b[39m, in \u001b[36mMobileViTBackbone.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/timm/models/byobnet.py:1761\u001b[39m, in \u001b[36mByobNet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1759\u001b[39m     x = checkpoint_seq(\u001b[38;5;28mself\u001b[39m.stages, x)\n\u001b[32m   1760\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1762\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_conv(x)\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/timm/models/mobilevit.py:263\u001b[39m, in \u001b[36mMobileVitBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    260\u001b[39m x = x.reshape(B, C, num_patches, \u001b[38;5;28mself\u001b[39m.patch_area).transpose(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).reshape(B * \u001b[38;5;28mself\u001b[39m.patch_area, num_patches, -\u001b[32m1\u001b[39m)\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m# Global representations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Fold (patch -> feature map)\u001b[39;00m\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# [B, P, N, C] --> [B*C*n_h, n_w, p_h, p_w]\u001b[39;00m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/timm/models/vision_transformer.py:205\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.ls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    206\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.ls2(\u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(x))))\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/timm/layers/attention.py:94\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x, attn_mask)\u001b[39m\n\u001b[32m     91\u001b[39m q, k = \u001b[38;5;28mself\u001b[39m.q_norm(q), \u001b[38;5;28mself\u001b[39m.k_norm(k)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fused_attn:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     x = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    100\u001b[39m     q = q * \u001b[38;5;28mself\u001b[39m.scale\n",
                        "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 4.00 GiB, other allocations: 12.21 GiB, max allowed: 20.13 GiB). Tried to allocate 6.10 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
                    ]
                }
            ],
            "source": [
                "# ===== CHANGE THIS FOR EACH PART =====\n",
                "CURRENT_UAV = \"A\"  # Options: \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"\n",
                "EPOCHS_THIS_PART = 1  # Epochs to train on this part\n",
                "# =====================================\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"üéØ TRAINING ON UAV-{CURRENT_UAV}\")\n",
                "print(f\"{'='*60}\\n\")\n",
                "\n",
                "# Step 1: Download this UAV part\n",
                "print(\"üì• Step 1: Downloading data...\")\n",
                "try:\n",
                "    uav_path = download_uav_part(CURRENT_UAV, output_dir=str(DATA_ROOT))\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Download failed: {e}\")\n",
                "    raise\n",
                "\n",
                "# Step 2: Create dataloaders for this UAV part\n",
                "print(f\"\\nüìÅ Step 2: Creating dataloaders for UAV-{CURRENT_UAV}...\")\n",
                "train_loader, val_loader, test_loader = create_dataloaders(\n",
                "    data_root=str(DATA_ROOT),\n",
                "    batch_size=config.data.batch_size,\n",
                "    num_workers=config.data.num_workers,\n",
                "    sequence_length=config.data.sequence_length,\n",
                "    stride=config.data.stride,\n",
                "    img_size=config.data.img_size,\n",
                "    sensor_type=config.data.sensor_type,\n",
                "    view=config.data.view,\n",
                "    uav_types=[CURRENT_UAV],  # Only train on this UAV part!\n",
                ")\n",
                "print(f\"   Train batches: {len(train_loader)}\")\n",
                "print(f\"   Val batches: {len(val_loader)}\")\n",
                "\n",
                "# Step 3: Setup trainer\n",
                "print(f\"\\nüèãÔ∏è Step 3: Setting up trainer...\")\n",
                "checkpoint_callback = ModelCheckpoint(\n",
                "    dirpath=str(CHECKPOINT_DIR),\n",
                "    filename=f\"uav-{CURRENT_UAV}-\" + \"{epoch:02d}-{val_loss:.4f}\",\n",
                "    save_top_k=1,\n",
                "    monitor=\"val_loss\",\n",
                "    mode=\"min\",\n",
                "    save_last=True,\n",
                ")\n",
                "\n",
                "early_stopping = EarlyStopping(\n",
                "    monitor=\"val_loss\",\n",
                "    patience=5,\n",
                "    mode=\"min\",\n",
                ")\n",
                "\n",
                "trainer = pl.Trainer(\n",
                "    max_epochs=EPOCHS_THIS_PART,\n",
                "    accelerator=\"auto\",\n",
                "    devices=1,\n",
                "    precision=\"16-mixed\" if DEVICE == \"cuda\" else 32,\n",
                "    callbacks=[checkpoint_callback, early_stopping],\n",
                "    enable_progress_bar=True,\n",
                "    gradient_clip_val=1.0,\n",
                "    log_every_n_steps=10,\n",
                ")\n",
                "\n",
                "# Step 4: Train!\n",
                "print(f\"\\nüöÄ Step 4: Training for {EPOCHS_THIS_PART} epochs...\")\n",
                "trainer.fit(model, train_loader, val_loader)\n",
                "\n",
                "# Step 5: Save checkpoint for next part\n",
                "print(f\"\\nüíæ Step 5: Saving checkpoint...\")\n",
                "trainer.save_checkpoint(str(LATEST_CHECKPOINT))\n",
                "print(f\"‚úÖ Saved to: {LATEST_CHECKPOINT}\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"‚úÖ FINISHED TRAINING ON UAV-{CURRENT_UAV}\")\n",
                "print(f\"   Best val_loss: {checkpoint_callback.best_model_score:.4f}\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='6-cleanup'></a>\n",
                "## 6. Cleanup (Optional)\n",
                "\n",
                "Delete downloaded data to free up storage before downloading the next UAV part."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Delete the UAV part we just trained on to free space\n",
                "cleanup_uav_part(CURRENT_UAV, data_dir=str(DATA_ROOT))\n",
                "\n",
                "# Verify deletion\n",
                "available = get_available_uav_types(str(DATA_ROOT))\n",
                "print(f\"\\nüìÅ Currently downloaded UAV types: {available if available else 'None'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìã Training Progress Tracker\n",
                "\n",
                "Keep track of which UAV parts you've trained on:\n",
                "\n",
                "| UAV | Status | Epochs | Notes |\n",
                "|-----|--------|--------|-------|\n",
                "| A   | ‚¨ú Not started |  |  |\n",
                "| B   | ‚¨ú Not started |  |  |\n",
                "| C   | ‚¨ú Not started |  |  |\n",
                "| D   | ‚¨ú Not started |  |  |\n",
                "| E   | ‚¨ú Not started |  |  |\n",
                "| F   | ‚¨ú Not started |  |  |\n",
                "\n",
                "After each training run, update the status to ‚úÖ Completed."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='7-evaluation'></a>\n",
                "## 7. Evaluation\n",
                "\n",
                "Evaluate the final model after training on all UAV parts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "if LATEST_CHECKPOINT.exists():\n",
                "    print(f\"üìÇ Loading final model from: {LATEST_CHECKPOINT}\")\n",
                "    model = MambaDetectorModule.load_from_checkpoint(\n",
                "        str(LATEST_CHECKPOINT),\n",
                "        config=config,\n",
                "    )\n",
                "    model.eval()\n",
                "    print(\"‚úÖ Model loaded for evaluation\")\n",
                "else:\n",
                "    print(\"‚ùå No checkpoint found. Train the model first.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions on a sample\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as patches\n",
                "import numpy as np\n",
                "\n",
                "# Make sure a UAV part is downloaded for testing\n",
                "available = get_available_uav_types(str(DATA_ROOT))\n",
                "if not available:\n",
                "    print(\"‚ö†Ô∏è No UAV data available. Download a part first.\")\n",
                "else:\n",
                "    # Create test loader with available data\n",
                "    _, _, test_loader = create_dataloaders(\n",
                "        data_root=str(DATA_ROOT),\n",
                "        batch_size=1,\n",
                "        num_workers=0,\n",
                "        sequence_length=config.data.sequence_length,\n",
                "        stride=config.data.stride,\n",
                "        img_size=config.data.img_size,\n",
                "        sensor_type=config.data.sensor_type,\n",
                "        view=config.data.view,\n",
                "        uav_types=available,\n",
                "    )\n",
                "    \n",
                "    # Get a sample batch\n",
                "    images, targets = next(iter(test_loader))\n",
                "    \n",
                "    # Run inference\n",
                "    with torch.no_grad():\n",
                "        predictions = model(images.to(DEVICE))\n",
                "    \n",
                "    # Visualize first frame of sequence\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "    \n",
                "    # Denormalize for visualization\n",
                "    mean = np.array([0.485, 0.456, 0.406])\n",
                "    std = np.array([0.229, 0.224, 0.225])\n",
                "    \n",
                "    for i, ax in enumerate(axes[:min(3, images.shape[1])]):\n",
                "        img = images[0, i].cpu().numpy().transpose(1, 2, 0)\n",
                "        img = (img * std + mean).clip(0, 1)\n",
                "        \n",
                "        ax.imshow(img)\n",
                "        ax.set_title(f\"Frame {i+1}\")\n",
                "        \n",
                "        # Draw predicted bbox\n",
                "        pred = predictions[0, i].cpu().numpy()\n",
                "        if pred[4] > 0.5:  # confidence threshold\n",
                "            x_center, y_center, w, h = pred[:4]\n",
                "            x_center *= config.data.img_size\n",
                "            y_center *= config.data.img_size\n",
                "            w *= config.data.img_size\n",
                "            h *= config.data.img_size\n",
                "            \n",
                "            rect = patches.Rectangle(\n",
                "                (x_center - w/2, y_center - h/2), w, h,\n",
                "                linewidth=2, edgecolor='r', facecolor='none'\n",
                "            )\n",
                "            ax.add_patch(rect)\n",
                "        \n",
                "        ax.axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(str(PROJECT_ROOT / 'outputs' / 'sample_prediction.png'), dpi=150)\n",
                "    plt.show()\n",
                "    print(f\"\\nüì∏ Saved visualization to: outputs/sample_prediction.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='8-export'></a>\n",
                "## 8. Export Model\n",
                "\n",
                "Export the trained model for deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to TorchScript\n",
                "EXPORT_DIR = PROJECT_ROOT / \"outputs\" / \"exported\"\n",
                "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "if LATEST_CHECKPOINT.exists():\n",
                "    model = MambaDetectorModule.load_from_checkpoint(\n",
                "        str(LATEST_CHECKPOINT),\n",
                "        config=config,\n",
                "    )\n",
                "    model.eval()\n",
                "    \n",
                "    # Create dummy input\n",
                "    dummy_input = torch.randn(\n",
                "        1, config.data.sequence_length, 3,\n",
                "        config.data.img_size, config.data.img_size\n",
                "    )\n",
                "    \n",
                "    # Export to TorchScript\n",
                "    print(\"üì¶ Exporting to TorchScript...\")\n",
                "    try:\n",
                "        scripted = torch.jit.trace(model.model, dummy_input)\n",
                "        torchscript_path = EXPORT_DIR / \"mamba_detector.pt\"\n",
                "        scripted.save(str(torchscript_path))\n",
                "        print(f\"‚úÖ Saved TorchScript model to: {torchscript_path}\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è TorchScript export failed: {e}\")\n",
                "        print(\"   This is expected with dynamic Mamba layers. Use checkpoint instead.\")\n",
                "    \n",
                "    # Also save as PyTorch checkpoint (more reliable)\n",
                "    torch_path = EXPORT_DIR / \"mamba_detector_final.ckpt\"\n",
                "    torch.save({\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'config': config,\n",
                "    }, torch_path)\n",
                "    print(f\"‚úÖ Saved PyTorch checkpoint to: {torch_path}\")\n",
                "else:\n",
                "    print(\"‚ùå No checkpoint found. Train the model first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéâ Training Complete!\n",
                "\n",
                "### Summary\n",
                "- Model trained on all UAV parts using incremental download-train-delete workflow\n",
                "- Final checkpoint saved to `outputs/checkpoints/latest.ckpt`\n",
                "- Exported model saved to `outputs/exported/`\n",
                "\n",
                "### Next Steps\n",
                "1. **Deploy to Lightning AI**: Upload the exported model for cloud inference\n",
                "2. **Hyperparameter Tuning**: Run `tune.py` for automated optimization\n",
                "3. **Multi-GPU Training**: Use `train.py` with DDP for faster training"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
