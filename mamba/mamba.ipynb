{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mamba UAV Detector - Training Notebook\n\nThis notebook contains all training and validation logic. Helper functions live in `mamba/` and `shared/`.",
   "id": "782586cc9a78213"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T14:55:16.559281Z",
     "start_time": "2026-01-29T14:55:12.264899Z"
    }
   },
   "source": "import os\nimport sys\nimport torch\nimport numpy as np\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n\nREPO_ROOT = os.path.abspath('..')\nif REPO_ROOT not in sys.path:\n    sys.path.append(REPO_ROOT)\n\nfrom mamba.config import Config\nfrom mamba.dataset import create_dataloaders\nfrom mamba.trainer import MambaDetectorModule\nfrom mamba.model import MambaUAVDetector\nimport shared.visualization as viz\n\npl.seed_everything(42)\n\nif torch.cuda.is_available():\n    device_name = torch.cuda.get_device_name(0)\nelif torch.backends.mps.is_available():\n    device_name = 'MPS'\nelse:\n    device_name = 'CPU'\n\nprint('✅ Setup complete')\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'Lightning version: {pl.__version__}')\nprint(f'Device: {device_name}')",
   "id": "6b87b5ef92da19ff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/.venv/lib/python3.13/site-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1028)>\n",
      "  data = fetch_version_info()\n",
      "/Users/ivantyshchenko/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/mamba_block.py:12: RuntimeWarning: mamba-ssm not available, using LSTM fallback\n",
      "  warnings.warn(\"mamba-ssm not available, using LSTM fallback\", RuntimeWarning)\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Setup complete\n",
      "PyTorch version: 2.10.0\n",
      "Lightning version: 2.6.0\n",
      "Device: MPS\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration",
   "id": "cfde599099069c56"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T14:55:59.670892Z",
     "start_time": "2026-01-29T14:55:59.655223Z"
    }
   },
   "source": "config = Config()\n\nconfig.data.data_root = '../data/MMFW-UAV/sample'\nconfig.data.sequence_length = 10\nconfig.data.batch_size = 4\n\nconfig.model.backbone = 'mobilevit_s'\nconfig.model.d_model = 256\nconfig.model.mamba_layers = 4\n\nconfig.training.max_epochs = 50\nconfig.training.lr = 1e-3\n\nconfig.experiment_name = 'mamba-mobilevit-s10'\nconfig.use_wandb = True\n\nif torch.cuda.is_available():\n    config.accelerator = 'gpu'\nelif torch.backends.mps.is_available():\n    config.accelerator = 'mps'\nelse:\n    config.accelerator = 'cpu'\n\nprint('Configuration:')\nprint(f'  Data root: {config.data.data_root}')\nprint(f'  Sequence length: {config.data.sequence_length}')\nprint(f'  Batch size: {config.data.batch_size}')\nprint(f'  Backbone: {config.model.backbone}')\nprint(f'  d_model: {config.model.d_model}')\nprint(f'  Mamba layers: {config.model.mamba_layers}')\nprint(f'  Max epochs: {config.training.max_epochs}')\nprint(f'  Accelerator: {config.accelerator}')",
   "id": "e8f305907cece20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Data root: ../data/MMFW-UAV/sample\n",
      "  Sequence length: 10\n",
      "  Batch size: 4\n",
      "  Backbone: mobilevit_s\n",
      "  d_model: 256\n",
      "  Mamba layers: 4\n",
      "  Max epochs: 50\n",
      "  Accelerator: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Loading",
   "id": "ffd1233414d9fb97"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T14:56:02.139260Z",
     "start_time": "2026-01-29T14:56:01.969243Z"
    }
   },
   "source": "train_loader, val_loader, test_loader = create_dataloaders(\n    data_root=config.data.data_root,\n    batch_size=config.data.batch_size,\n    num_workers=config.data.num_workers,\n    sequence_length=config.data.sequence_length,\n    sensor_type=config.data.sensor_type,\n    view=config.data.view,\n    img_size=config.data.img_size,\n    stride=config.data.stride,\n)\n\nprint('✅ Data loaded:')\nprint(f'  Train batches: {len(train_loader)}')\nprint(f'  Val batches: {len(val_loader)}')\nprint(f'  Test batches: {len(test_loader)}')\n\nimages, targets = next(iter(train_loader))\nprint('\\nBatch shape:')\nprint(f'  Images: {images.shape}')\nprint(f'  Targets: {targets.shape}')",
   "id": "14468f81b5ed10a0",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Split file not found: ../data/MMFW-UAV/splits/train.json. Run scripts/prepare_data.py to generate splits.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m train_loader, val_loader, test_loader = \u001B[43mcreate_dataloaders\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43msensor_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43msensor_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mview\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mimg_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimg_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33m✅ Data loaded:\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     13\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m  Train batches: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/dataset.py:156\u001B[39m, in \u001B[36mcreate_dataloaders\u001B[39m\u001B[34m(data_root, batch_size, num_workers, sequence_length, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_dataloaders\u001B[39m(\n\u001B[32m    149\u001B[39m     data_root: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m    150\u001B[39m     batch_size: \u001B[38;5;28mint\u001B[39m = \u001B[32m4\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    153\u001B[39m     **kwargs,\n\u001B[32m    154\u001B[39m ):\n\u001B[32m    155\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create train/val/test dataloaders.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m156\u001B[39m     train_dataset = \u001B[43mMMFWUAVSequenceDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata_root\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtrain\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43msequence_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    163\u001B[39m     val_dataset = MMFWUAVSequenceDataset(\n\u001B[32m    164\u001B[39m         data_root=data_root,\n\u001B[32m    165\u001B[39m         split=\u001B[33m\"\u001B[39m\u001B[33mval\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    166\u001B[39m         sequence_length=sequence_length,\n\u001B[32m    167\u001B[39m         **kwargs,\n\u001B[32m    168\u001B[39m     )\n\u001B[32m    170\u001B[39m     test_dataset = MMFWUAVSequenceDataset(\n\u001B[32m    171\u001B[39m         data_root=data_root,\n\u001B[32m    172\u001B[39m         split=\u001B[33m\"\u001B[39m\u001B[33mtest\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    173\u001B[39m         sequence_length=sequence_length,\n\u001B[32m    174\u001B[39m         **kwargs,\n\u001B[32m    175\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/Python/Hot-Peppers-Company-Computer-Vision/mamba/dataset.py:42\u001B[39m, in \u001B[36mMMFWUAVSequenceDataset.__init__\u001B[39m\u001B[34m(self, data_root, split, sensor_type, view, sequence_length, stride, img_size, transform)\u001B[39m\n\u001B[32m     40\u001B[39m split_file = \u001B[38;5;28mself\u001B[39m.data_root.parent / \u001B[33m\"\u001B[39m\u001B[33msplits\u001B[39m\u001B[33m\"\u001B[39m / \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.json\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m split_file.exists():\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[32m     43\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSplit file not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msplit_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     44\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mRun scripts/prepare_data.py to generate splits.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     45\u001B[39m     )\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(split_file, \u001B[33m\"\u001B[39m\u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     47\u001B[39m     \u001B[38;5;28mself\u001B[39m.split_data = json.load(f)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: Split file not found: ../data/MMFW-UAV/splits/train.json. Run scripts/prepare_data.py to generate splits."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Sample Sequence",
   "id": "5e230fedfe633e21"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nsample_seq = images[0]\nsample_targets = targets[0]\n\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\naxes = axes.flatten()\n\nfor i in range(min(10, len(sample_seq))):\n    img = sample_seq[i].permute(1, 2, 0).numpy()\n    img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n    img = np.clip(img, 0, 1)\n\n    axes[i].imshow(img)\n    axes[i].set_title(f'Frame {i}')\n    axes[i].axis('off')\n\n    target = sample_targets[i]\n    x, y, w, h, conf = target\n    if conf > 0.5:\n        rect_x = (x - w / 2) * config.data.img_size\n        rect_y = (y - h / 2) * config.data.img_size\n        rect_w = w * config.data.img_size\n        rect_h = h * config.data.img_size\n        rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, edgecolor='red', linewidth=2)\n        axes[i].add_patch(rect)\n\nplt.tight_layout()\nplt.show()",
   "id": "acbdf84fdad7860f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Initialize Model",
   "id": "5ce636efd99073cd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "model = MambaDetectorModule(config)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('✅ Model created:')\nprint(f'  Total parameters: {total_params:,}')\nprint(f'  Trainable parameters: {trainable_params:,}')\n\nwith torch.no_grad():\n    test_output = model(images[:2])\n    print('\\nTest forward pass:')\n    print(f'  Input shape: {images[:2].shape}')\n    print(f'  Output shape: {test_output.shape}')",
   "id": "6b3bec0e39c26399"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Training",
   "id": "dcbde5c53c7e3271"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "os.makedirs('../outputs/checkpoints', exist_ok=True)\n\ncallbacks = [\n    ModelCheckpoint(\n        dirpath='../outputs/checkpoints',\n        filename=f'{config.experiment_name}-{{epoch:02d}}-{{val_loss:.3f}}',\n        monitor=config.training.monitor,\n        mode=config.training.mode,\n        save_top_k=config.training.save_top_k,\n        save_last=True,\n    ),\n    EarlyStopping(\n        monitor=config.training.monitor,\n        patience=10,\n        mode=config.training.mode,\n    ),\n    LearningRateMonitor(logging_interval='epoch'),\n]\n\nif config.use_wandb:\n    logger = WandbLogger(\n        project=config.project_name,\n        name=config.experiment_name,\n        log_model=True,\n    )\nelse:\n    logger = True\n\ntrainer = pl.Trainer(\n    max_epochs=config.training.max_epochs,\n    accelerator=config.accelerator,\n    devices=config.devices,\n    callbacks=callbacks,\n    logger=logger,\n    log_every_n_steps=config.training.log_every_n_steps,\n    val_check_interval=config.training.val_check_interval,\n    gradient_clip_val=config.training.gradient_clip_val,\n    deterministic=True,\n)\n\nprint('✅ Trainer configured')",
   "id": "28eb6f3b91562e13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train Model",
   "id": "901e39c2ed511f72"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "trainer.fit(model, train_loader, val_loader)\n\nprint('✅ Training complete!')\nprint(f'Best model: {trainer.checkpoint_callback.best_model_path}')",
   "id": "3df3d6f5ebae135a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Evaluation",
   "id": "a2bc58f0091adcea"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "best_model_path = trainer.checkpoint_callback.best_model_path\nmodel = MambaDetectorModule.load_from_checkpoint(best_model_path, config=config)\nmodel.eval()\n\ntrainer.test(model, test_loader)",
   "id": "1a4c4ad50d39735f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Inference on Sample",
   "id": "c97619158c910556"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "images, targets = next(iter(test_loader))\nsample_img = images[0]\nsample_target = targets[0]\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(device)\nsample_img = sample_img.to(device)\n\nwith torch.no_grad():\n    pred_dict = model.model.predict(sample_img.unsqueeze(0))\n\nprint('Prediction:')\nprint(f\"  Bbox: {pred_dict['bbox'][0]}\")\nprint(f\"  Confidence: {pred_dict['confidence'][0]:.3f}\")\n\nprint('\\nGround Truth:')\nprint(f\"  Bbox: {sample_target[-1, :4]}\")\nprint(f\"  Confidence: {sample_target[-1, 4]:.3f}\")\n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nimg_gt = sample_img[-1].detach().cpu().permute(1, 2, 0).numpy()\nimg_gt = img_gt * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\nimg_gt = np.clip(img_gt, 0, 1)\nplt.imshow(img_gt)\nplt.title('Ground Truth')\n\nx, y, w, h, conf = sample_target[-1].cpu()\nif conf > 0.5:\n    rect_x = (x - w / 2) * config.data.img_size\n    rect_y = (y - h / 2) * config.data.img_size\n    rect_w = w * config.data.img_size\n    rect_h = h * config.data.img_size\n    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, edgecolor='green', linewidth=2)\n    plt.gca().add_patch(rect)\n\nplt.subplot(1, 2, 2)\nplt.imshow(img_gt)\nplt.title('Prediction')\n\npred_bbox = pred_dict['bbox'][0].detach().cpu()\npred_conf = pred_dict['confidence'][0].detach().cpu()\nx, y, w, h = pred_bbox\nif pred_conf > 0.5:\n    rect_x = (x - w / 2) * config.data.img_size\n    rect_y = (y - h / 2) * config.data.img_size\n    rect_w = w * config.data.img_size\n    rect_h = h * config.data.img_size\n    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, edgecolor='red', linewidth=2)\n    plt.gca().add_patch(rect)\n\nplt.tight_layout()\nplt.show()",
   "id": "1b2dc9e1174c858d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Export Model",
   "id": "1f685fa5292291fd"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "os.makedirs('../outputs', exist_ok=True)\nscripted_model = torch.jit.script(model.model)\ntorch.jit.save(scripted_model, f'../outputs/{config.experiment_name}.pt')\nprint(f\"✅ Model exported to: ../outputs/{config.experiment_name}.pt\")\n\ndummy_input = torch.randn(1, config.data.sequence_length, 3, config.data.img_size, config.data.img_size)\ntorch.onnx.export(\n    model.model,\n    dummy_input,\n    f'../outputs/{config.experiment_name}.onnx',\n    export_params=True,\n    opset_version=11,\n    input_names=['input'],\n    output_names=['output'],\n)\nprint(f\"✅ Model exported to: ../outputs/{config.experiment_name}.onnx\")",
   "id": "b6a4ef22f0aba325"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
